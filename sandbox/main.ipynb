{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0930c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Версия PyTorch: 2.7.1+cu118\n",
      "Версия CUDA (если установлена): 11.8\n",
      "Версия cuDNN: 90100\n",
      "Number of GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Версия PyTorch: {torch.__version__}\")\n",
    "    print(f\"Версия CUDA (если установлена): {torch.version.cuda}\")\n",
    "    print(f\"Версия cuDNN: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    device_str = 'cpu'\n",
    "    print(\"CUDA not available. Using device: cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7326b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fd7cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.31'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3df28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d559a581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95eac044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the BSD license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# A MinGPT + Lightning + xFormers example Code from Sean Naren (@seannaren)\n",
    "# This is an hommage to https://github.com/karpathy/minGPT\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import seed_everything, Trainer\n",
    "from pytorch_lightning.utilities import rank_zero_info\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe0dbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xFormers 0.0.31\n",
      "memory_efficient_attention.ckF:                    unavailable\n",
      "memory_efficient_attention.ckB:                    unavailable\n",
      "memory_efficient_attention.ck_decoderF:            unavailable\n",
      "memory_efficient_attention.ck_splitKF:             unavailable\n",
      "memory_efficient_attention.cutlassF-pt:            available\n",
      "memory_efficient_attention.cutlassB-pt:            available\n",
      "memory_efficient_attention.fa2F@2.5.7-pt:          available\n",
      "memory_efficient_attention.fa2B@2.5.7-pt:          available\n",
      "memory_efficient_attention.fa3F@0.0.0:             unavailable\n",
      "memory_efficient_attention.fa3B@0.0.0:             unavailable\n",
      "memory_efficient_attention.fa3F_splitKV@0.0.0:     unavailable\n",
      "memory_efficient_attention.triton_splitKF:         available\n",
      "indexing.scaled_index_addF:                        available\n",
      "indexing.scaled_index_addB:                        available\n",
      "indexing.index_select:                             available\n",
      "sp24.sparse24_sparsify_both_ways:                  available\n",
      "sp24.sparse24_apply:                               available\n",
      "sp24.sparse24_apply_dense_output:                  available\n",
      "sp24._sparse24_gemm:                               available\n",
      "sp24._cslt_sparse_mm_search@0.4.0:                 available\n",
      "sp24._cslt_sparse_mm@0.4.0:                        available\n",
      "swiglu.dual_gemm_silu:                             available\n",
      "swiglu.gemm_fused_operand_sum:                     available\n",
      "swiglu.fused.p.cpp:                                available\n",
      "is_triton_available:                               True\n",
      "pytorch.version:                                   2.7.1+cu118\n",
      "pytorch.cuda:                                      available\n",
      "gpu.compute_capability:                            8.0\n",
      "gpu.name:                                          NVIDIA A100-SXM4-80GB\n",
      "dcgm_profiler:                                     unavailable\n",
      "build.info:                                        available\n",
      "build.cuda_version:                                1108\n",
      "build.hip_version:                                 None\n",
      "build.python_version:                              3.9.23\n",
      "build.torch_version:                               2.7.1+cu118\n",
      "build.env.TORCH_CUDA_ARCH_LIST:                    7.5 8.0+PTX\n",
      "build.env.PYTORCH_ROCM_ARCH:                       None\n",
      "build.env.XFORMERS_BUILD_TYPE:                     Release\n",
      "build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS:        None\n",
      "build.env.NVCC_FLAGS:                              -allow-unsupported-compiler\n",
      "build.env.XFORMERS_PACKAGE_FROM:                   wheel-v0.0.31\n",
      "build.nvcc_version:                                11.8.89\n",
      "source.privacy:                                    open source\n"
     ]
    }
   ],
   "source": [
    "!poetry run python -m xformers.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a56884db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xformers.factory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfactory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m xFormer, xFormerConfig, xFormerEncoderConfig\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xformers.factory'"
     ]
    }
   ],
   "source": [
    "from xformers.factory import xFormer, xFormerConfig, xFormerEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MolecularEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_layers=6, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        encoder_layer_template = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            batch_first=True \n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(  # <--- xformer in env\n",
    "            encoder_layer=encoder_layer_template,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Linear(d_model, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformer-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
