{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6a9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import dask\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547f40a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      "  pubchem Parquet: \tpubchem_compounds.parquet.dask\n",
      "  drugspacex Parquet: \tdrugspacex.parquet.dask\n"
     ]
    }
   ],
   "source": [
    "# --- Конфигурация путей ---\n",
    "DATA_DIR_PUBCH = Path(\"../data/pubchem_compounds.parquet.dask\")\n",
    "DATA_DIR_DRUGS = Path(\"../data/drugspacex.parquet.dask\")\n",
    "\n",
    "\n",
    "print(\"config:\")\n",
    "print(f\"  pubchem Parquet: \\t{DATA_DIR_PUBCH.name}\")\n",
    "print(f\"  drugspacex Parquet: \\t{DATA_DIR_DRUGS.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706c9a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк в PubChem: 121458159\n",
      "Количество строк в DrugsSpaceX: 100946534\n"
     ]
    }
   ],
   "source": [
    "# Загружаем датафреймы\n",
    "ddf_pubch = dd.read_parquet(DATA_DIR_PUBCH)\n",
    "ddf_drugs = dd.read_parquet(DATA_DIR_DRUGS)\n",
    "\n",
    "# Вычисляем и выводим количество строк\n",
    "len_pubch, len_drugs = dask.compute(len(ddf_pubch), len(ddf_drugs))\n",
    "# или\n",
    "# len_pubch = ddf_pubch.shape[0].compute()\n",
    "# len_drugs = ddf_drugs.shape[0].compute()\n",
    "\n",
    "\n",
    "print(f\"Количество строк в PubChem: {len_pubch}\")\n",
    "print(f\"Количество строк в DrugsSpaceX: {len_drugs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d45092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных 'smiles' в PubChem: 121401187\n",
      "Количество уникальных 'smiles' в DrugsSpaceX: 100946534\n"
     ]
    }
   ],
   "source": [
    "# Предположим, нас интересует колонка 'smiles' в PubChem и 'smiles' в DrugsSpaceX\n",
    "unique_pubch = ddf_pubch['smiles'].nunique().compute()\n",
    "unique_drugs = ddf_drugs['smiles'].nunique().compute()\n",
    "\n",
    "print(f\"Количество уникальных 'smiles' в PubChem: {unique_pubch}\")\n",
    "print(f\"Количество уникальных 'smiles' в DrugsSpaceX: {unique_drugs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f815ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56972"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_pubch - unique_pubch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6492e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_drugs - unique_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695fa6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CC(=O)OC(CC(=O)O)C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C1=CC(C(C(=C1)C(=O)O)O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CC(CN)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C(C(=O)COP(=O)(O)O)N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cid                            smiles\n",
       "0   1  CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
       "1   2     CC(=O)OC(CC(=O)O)C[N+](C)(C)C\n",
       "2   3          C1=CC(C(C(=C1)C(=O)O)O)O\n",
       "3   4                           CC(CN)O\n",
       "4   5              C(C(=O)COP(=O)(O)O)N"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_pubch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "406ecafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>de_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC[C@H](C)[C@H](NC(=O)[C@H](CCC(=O)O)NC(=O)[C@...</td>\n",
       "      <td>DE1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...</td>\n",
       "      <td>DE2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...</td>\n",
       "      <td>DE3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N=C(N)NCCC[C@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1...</td>\n",
       "      <td>DE4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(=O)N[C@H](Cc1ccc2ccccc2c1)C(=O)N[C@H](Cc1cc...</td>\n",
       "      <td>DE5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles de_id\n",
       "0  CC[C@H](C)[C@H](NC(=O)[C@H](CCC(=O)O)NC(=O)[C@...   DE1\n",
       "1  CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...   DE2\n",
       "2  CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...   DE3\n",
       "3  N=C(N)NCCC[C@H](NC(=O)[C@@H]1CCCN1C(=O)[C@@H]1...   DE4\n",
       "4  CC(=O)N[C@H](Cc1ccc2ccccc2c1)C(=O)N[C@H](Cc1cc...   DE5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_drugs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1376ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard link: http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 13:04:59,592 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 61643a02da55786aa69022f2aa97713e initialized by task ('shuffle-transfer-61643a02da55786aa69022f2aa97713e', 109) executed on worker tcp://127.0.0.1:45063\n",
      "2025-07-11 13:04:59,611 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b610ee960b453d7baccd7514be14671e initialized by task ('shuffle-transfer-b610ee960b453d7baccd7514be14671e', 110) executed on worker tcp://127.0.0.1:45063\n",
      "2025-07-11 13:06:18,586 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 41afcc07a81f0a1624906ac75163bb1f initialized by task ('hash-join-transfer-41afcc07a81f0a1624906ac75163bb1f', 23) executed on worker tcp://127.0.0.1:34575\n",
      "2025-07-11 13:06:19,710 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle ee8db639613c3d69b88233ec77d35125 initialized by task ('hash-join-transfer-ee8db639613c3d69b88233ec77d35125', 102) executed on worker tcp://127.0.0.1:44839\n",
      "2025-07-11 13:06:20,783 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 61643a02da55786aa69022f2aa97713e deactivated due to stimulus 'task-finished-1752239179.4453354'\n",
      "2025-07-11 13:06:26,729 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b610ee960b453d7baccd7514be14671e deactivated due to stimulus 'task-finished-1752239185.919651'\n",
      "2025-07-11 13:07:47,823 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle ee8db639613c3d69b88233ec77d35125 deactivated due to stimulus 'task-finished-1752239267.8205607'\n",
      "2025-07-11 13:07:47,824 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 41afcc07a81f0a1624906ac75163bb1f deactivated due to stimulus 'task-finished-1752239267.8205607'\n",
      "2025-07-11 13:07:50,319 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b610ee960b453d7baccd7514be14671e initialized by task ('shuffle-transfer-b610ee960b453d7baccd7514be14671e', 106) executed on worker tcp://127.0.0.1:44157\n",
      "2025-07-11 13:08:33,798 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b610ee960b453d7baccd7514be14671e deactivated due to stimulus 'task-finished-1752239313.4267817'\n",
      "2025-07-11 13:08:37,178 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 61643a02da55786aa69022f2aa97713e initialized by task ('shuffle-transfer-61643a02da55786aa69022f2aa97713e', 104) executed on worker tcp://127.0.0.1:35541\n",
      "2025-07-11 13:09:19,006 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 61643a02da55786aa69022f2aa97713e deactivated due to stimulus 'task-finished-1752239359.0043733'\n",
      "2025-07-11 13:10:21,553 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 61643a02da55786aa69022f2aa97713e initialized by task ('shuffle-transfer-61643a02da55786aa69022f2aa97713e', 83) executed on worker tcp://127.0.0.1:33863\n",
      "2025-07-11 13:10:21,555 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b610ee960b453d7baccd7514be14671e initialized by task ('shuffle-transfer-b610ee960b453d7baccd7514be14671e', 107) executed on worker tcp://127.0.0.1:33863\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-394784' coro=<Client._gather.<locals>.wait() done, defined at /app/.venv/lib/python3.11/site-packages/distributed/client.py:2377> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/.venv/lib/python3.11/site-packages/distributed/client.py\", line 2386, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n",
      "2025-07-11 13:10:29,247 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 61643a02da55786aa69022f2aa97713e deactivated due to stimulus 'client-releases-keys-1752239429.2355816'\n",
      "2025-07-11 13:10:29,267 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle b610ee960b453d7baccd7514be14671e deactivated due to stimulus 'client-releases-keys-1752239429.2355816'\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-476641' coro=<Client._gather.<locals>.wait() done, defined at /app/.venv/lib/python3.11/site-packages/distributed/client.py:2377> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/app/.venv/lib/python3.11/site-packages/distributed/client.py\", line 2386, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Конфигурация, как у вас ---\n",
    "DATA_DIR_PUBCH = Path(\"../data/pubchem_compounds.parquet.dask\")\n",
    "DATA_DIR_DRUGS = Path(\"../data/drugspacex.parquet.dask\")\n",
    "client = Client() # Запускаем клиент\n",
    "print(f\"Dask dashboard link: {client.dashboard_link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607c6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Загружаем данные\n",
    "ddf_pubch = dd.read_parquet(DATA_DIR_PUBCH)\n",
    "ddf_drugs = dd.read_parquet(DATA_DIR_DRUGS)\n",
    "\n",
    "# --- Шаг 1 и 2: Извлекаем и находим уникальные идентификаторы ---\n",
    "# Работаем только с одной колонкой, что сильно экономит память\n",
    "unique_smiles_pubch = ddf_pubch[['smiles']].drop_duplicates()\n",
    "unique_smiles_drugs = ddf_drugs[['smiles']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cfc74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем вычисление пересечения и общего количества...\n",
      "Вычисления завершены.\n",
      "\n",
      "Общее число уникальных молекул в PubChem: 121401187\n",
      "Общее число уникальных молекул в DrugsSpaceX: 100946534\n",
      "Найдено общих уникальных молекул между датасетами: 37942\n",
      "Общие молекулы составляют 0.03% от уникальных молекул в PubChem.\n",
      "Общие молекулы составляют 0.04% от уникальных молекул в DrugsSpaceX.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Шаг 3: Находим пересечение уникальных идентификаторов ---\n",
    "# Это самый быстрый способ найти общие молекулы\n",
    "intersecting_smiles = dd.merge(\n",
    "    unique_smiles_pubch,\n",
    "    unique_smiles_drugs,\n",
    "    on='smiles',  # Колонка для сравнения\n",
    "    how='inner'  # 'inner' означает \"пересечение\"\n",
    ")\n",
    "\n",
    "# --- Шаг 4: Готовим вычисления ---\n",
    "# Мы хотим посчитать 3 значения:\n",
    "# 1. Количество общих молекул (размер пересечения)\n",
    "# 2. Общее количество уникальных молекул в PubChem\n",
    "# 3. Общее количество уникальных молекул в DrugsSpaceX\n",
    "count_intersection = len(intersecting_smiles)\n",
    "count_unique_pubch = len(unique_smiles_pubch)\n",
    "count_unique_drugs = len(unique_smiles_drugs)\n",
    "\n",
    "# --- Шаг 5: Запускаем все вычисления одной командой ---\n",
    "# Dask оптимизирует и выполнит их параллельно\n",
    "print(\"Начинаем вычисление пересечения и общего количества...\")\n",
    "num_intersection, total_unique_pubch, total_unique_drugs = dask.compute(\n",
    "    count_intersection,\n",
    "    count_unique_pubch,\n",
    "    count_unique_drugs\n",
    ")\n",
    "print(\"Вычисления завершены.\")\n",
    "\n",
    "# --- Рассчитываем и выводим результаты ---\n",
    "print(f\"\\nОбщее число уникальных молекул в PubChem: {total_unique_pubch}\")\n",
    "print(f\"Общее число уникальных молекул в DrugsSpaceX: {total_unique_drugs}\")\n",
    "print(f\"Найдено общих уникальных молекул между датасетами: {num_intersection}\")\n",
    "\n",
    "# Рассчитываем проценты\n",
    "if total_unique_pubch > 0:\n",
    "    percent_vs_pubchem = (num_intersection / total_unique_pubch) * 100\n",
    "    print(f\"Общие молекулы составляют {percent_vs_pubchem:.2f}% от уникальных молекул в PubChem.\")\n",
    "\n",
    "if total_unique_drugs > 0:\n",
    "    percent_vs_drugs = (num_intersection / total_unique_drugs) * 100\n",
    "    print(f\"Общие молекулы составляют {percent_vs_drugs:.2f}% от уникальных молекул в DrugsSpaceX.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be408486",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb24f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка наличия файлов для теста ---\n",
      "✔️  CSV файл найден: ../data/CID-SMILES\n",
      "✔️  Parquet директория найдена: ../data/pubchem_compounds.parquet.dask\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- ШАГ 1: КОНФИГУРАЦИЯ И ПРОВЕРКА ПУТЕЙ ---\n",
    "DATA_DIR = Path(\"../data\")\n",
    "CSV_PATH = DATA_DIR / \"CID-SMILES\"\n",
    "# Dask сохраняет паркет в директорию, pandas/pyarrow умеют читать ее напрямую\n",
    "PARQUET_PATH = DATA_DIR / \"pubchem_compounds.parquet.dask\"\n",
    "\n",
    "print(\"--- Проверка наличия файлов для теста ---\")\n",
    "\n",
    "# Проверяем, что оба файла/директории существуют\n",
    "if not CSV_PATH.exists():\n",
    "    print(f\"ОШИБКА: CSV файл не найден по пути: {CSV_PATH}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"✔️  CSV файл найден: {CSV_PATH}\")\n",
    "\n",
    "if not PARQUET_PATH.exists():\n",
    "    print(f\"ОШИБКА: Директория Parquet не найдена по пути: {PARQUET_PATH}\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"✔️  Parquet директория найдена: {PARQUET_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1565504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CC(=O)OC(CC(=O)O)C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C1=CC(C(C(=C1)C(=O)O)O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CC(CN)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C(C(=O)COP(=O)(O)O)N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cid                            smiles\n",
       "0   1  CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
       "1   2     CC(=O)OC(CC(=O)O)C[N+](C)(C)C\n",
       "2   3          C1=CC(C(C(=C1)C(=O)O)O)O\n",
       "3   4                           CC(CN)O\n",
       "4   5              C(C(=O)COP(=O)(O)O)N"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddf = dd.read_csv(\n",
    "    CSV_PATH, \n",
    "    sep='\\t', \n",
    "    header=None,                  \n",
    "    names=['cid', 'smiles'],      \n",
    "    dtype={'cid': 'str', 'smiles': 'str'}, \n",
    "    blocksize='64MB'\n",
    ")\n",
    "display(ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ec8c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CC(=O)OC(CC(=O)O)C[N+](C)(C)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>C1=CC(C(C(=C1)C(=O)O)O)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CC(CN)O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C(C(=O)COP(=O)(O)O)N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cid                            smiles\n",
       "0   1  CC(=O)OC(CC(=O)[O-])C[N+](C)(C)C\n",
       "1   2     CC(=O)OC(CC(=O)O)C[N+](C)(C)C\n",
       "2   3          C1=CC(C(C(=C1)C(=O)O)O)O\n",
       "3   4                           CC(CN)O\n",
       "4   5              C(C(=O)COP(=O)(O)O)N"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    CSV_PATH, \n",
    "    sep='\\t', \n",
    "    header=None,                  \n",
    "    names=['cid', 'smiles'],      \n",
    "    dtype={'cid': 'str', 'smiles': 'str'}\n",
    ")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc00890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Тестирование производительности для формата: CSV ---\n",
      "1. Загрузка данных в память...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_duration, iter_duration\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# --- ШАГ 4: ЗАПУСК И ВЫВОД РЕЗУЛЬТАТОВ (БЕЗ ИЗМЕНЕНИЙ) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m csv_load_time, csv_iter_time = \u001b[43mrun_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCsvSmilesDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCSV\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m parquet_load_time, parquet_iter_time = run_benchmark(ParquetSmilesDataset, PARQUET_PATH, \u001b[33m\"\u001b[39m\u001b[33mParquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m csv_size_gb = os.path.getsize(CSV_PATH) / (\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrun_benchmark\u001b[39m\u001b[34m(dataset_class, path, format_name)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m1. Загрузка данных в память...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m start_load_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m dataset = \u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m load_duration = time.time() - start_load_time\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ...завершено за \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_duration\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m секунд.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mCsvSmilesDataset.__init__\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# --- ИЗМЕНЕНИЕ ЗДЕСЬ ---\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Применяем предоставленные вами, правильные параметры для чтения.\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# usecols=['smiles'] все еще важен - он говорит pandas загрузить в память\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# только данные из колонки, которую мы назвали 'smiles'.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28mself\u001b[39m.smiles = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msmiles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msmiles\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Указываем имя, которое мы только что присвоили\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33msmiles\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m1000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- ШАГ 2: СОЗДАНИЕ КЛАССОВ DATASET ---\n",
    "\n",
    "class CsvSmilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Читает колонку 'smiles' из большого CSV файла БЕЗ ЗАГОЛОВКА и с табуляцией.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        # --- ИЗМЕНЕНИЕ ЗДЕСЬ ---\n",
    "        # Применяем предоставленные вами, правильные параметры для чтения.\n",
    "        # usecols=['smiles'] все еще важен - он говорит pandas загрузить в память\n",
    "        # только данные из колонки, которую мы назвали 'smiles'.\n",
    "        self.smiles = pd.read_csv(\n",
    "            path,\n",
    "            sep='\\t',\n",
    "            header=None,\n",
    "            names=['cid', 'smiles'],\n",
    "            usecols=['smiles']  # Указываем имя, которое мы только что присвоили\n",
    "        )['smiles'].head(1000)\n",
    "\n",
    "\n",
    "class ParquetSmilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Читает колонку 'smiles' из директории с Parquet файлами.\n",
    "    КОД НЕ ИЗМЕНИЛСЯ - Parquet хранит свою схему и не нуждается во внешних подсказках.\n",
    "    \"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.smiles = pd.read_parquet(path, columns=['smiles'])['smiles']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smiles.iloc[idx]\n",
    "\n",
    "\n",
    "# --- ШАГ 3: ФУНКЦИЯ ДЛЯ БЕНЧМАРКА (БЕЗ ИЗМЕНЕНИЙ) ---\n",
    "\n",
    "def run_benchmark(dataset_class, path, format_name):\n",
    "    print(f\"\\n--- Тестирование производительности для формата: {format_name} ---\")\n",
    "    print(\"1. Загрузка данных в память...\")\n",
    "    start_load_time = time.time()\n",
    "    dataset = dataset_class(path)\n",
    "    load_duration = time.time() - start_load_time\n",
    "    print(f\"   ...завершено за {load_duration:.2f} секунд.\")\n",
    "\n",
    "    print(\"2. Итерация по данным (1 эпоха)...\")\n",
    "    dataloader = DataLoader(dataset, batch_size=512, num_workers=4, shuffle=False)\n",
    "    \n",
    "    start_iter_time = time.time()\n",
    "    for _ in tqdm(dataloader, desc=f\"Прогон {format_name}\"):\n",
    "        pass\n",
    "    iter_duration = time.time() - start_iter_time\n",
    "    print(f\"   ...завершено за {iter_duration:.2f} секунд.\")\n",
    "\n",
    "    return load_duration, iter_duration\n",
    "\n",
    "\n",
    "# --- ШАГ 4: ЗАПУСК И ВЫВОД РЕЗУЛЬТАТОВ (БЕЗ ИЗМЕНЕНИЙ) ---\n",
    "\n",
    "csv_load_time, csv_iter_time = run_benchmark(CsvSmilesDataset, CSV_PATH, \"CSV\")\n",
    "parquet_load_time, parquet_iter_time = run_benchmark(ParquetSmilesDataset, PARQUET_PATH, \"Parquet\")\n",
    "\n",
    "csv_size_gb = os.path.getsize(CSV_PATH) / (1024**3)\n",
    "parquet_size_gb = sum(f.stat().st_size for f in PARQUET_PATH.glob('*.parquet')) / (1024**3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" \" * 15 + \"ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Параметр':<25} {'CSV':>10} {'Parquet':>12}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Размер файла (GB)':<25} {csv_size_gb:>9.2f} G {parquet_size_gb:>11.2f} G\")\n",
    "print(f\"{'Время загрузки (сек)':<25} {csv_load_time:>9.2f} с {parquet_load_time:>11.2f} с\")\n",
    "print(f\"{'Время эпохи (сек)':<25} {csv_iter_time:>9.2f} с {parquet_iter_time:>11.2f} с\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "if parquet_load_time > 0:\n",
    "    print(f\"\\n✔️  Загрузка данных из Parquet оказалась быстрее в {csv_load_time / parquet_load_time:.1f} раз.\")\n",
    "if csv_size_gb > 0:\n",
    "    print(f\"✔️  Parquet занимает на диске на {100 * (1 - parquet_size_gb/csv_size_gb):.1f}% меньше места.\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformer-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
